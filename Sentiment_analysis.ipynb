{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdc92d27-abc5-4e77-8048-0ff8419747ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/25 09:25:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/03/25 09:25:29 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "22/03/25 09:25:31 WARN ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    }
   ],
   "source": [
    "import pyspark as ps\n",
    "from pyspark.sql import SparkSession,SQLContext\n",
    "from operator import add\n",
    "from pyspark import SparkConf\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "import warnings\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.6:7077\") \\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"300s\")\\\n",
    "        .config(\"spark.executor.cores\",3)\\\n",
    "        .config(\"spark.driver.port\",9998)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .appName(\"Group2\")\\\n",
    "        .getOrCreate()\n",
    "sc = spark_session.sparkContext\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eacd88a-da77-4a93-9f7a-c9bbb57644b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/25 09:23:15 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/03/25 09:23:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/03/25 09:23:45 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/03/25 09:24:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/03/25 09:24:15 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/03/25 09:24:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/03/25 09:24:45 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/03/25 09:25:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/03/25 09:25:15 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"hdfs://192.168.2.6:9000/user/ubuntu/input/RC_2005-12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c09f847-01d3-4077-8ae9-5effbe4aa1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"controversiality\").distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f96b640-ff84-4c6e-b45b-5408d71332b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe40ffb-b9b9-4378-9990-e669f2191a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6610c4eb-a58a-4234-a3c3-baf0fac306cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df.select(\"body\",\"controversiality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26400461-0242-4f64-b97d-75729ac30f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbac0c0-8f8b-45bd-bb61-eb6aeb980081",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.rdd.getNumPartitions()\n",
    "df_1.repartition(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0150955-9be8-44c9-a89d-63727701cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_set, val_set, test_set) = df_1.randomSplit([0.98, 0.01, 0.01], seed = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405b9c4e-ca96-4f0a-8d09-a40454a6cf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d8bcde-6d7d-453b-881a-a914964b1373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"body\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**12, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"controversiality\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])\n",
    "\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "train_df = pipelineFit.transform(train_set)\n",
    "val_df = pipelineFit.transform(val_set)\n",
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50dbd8d-057d-4d8d-beb1-89b09a69cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "lrModel = lr.fit(train_df)\n",
    "end = time.time()\n",
    "print(\"time taken for training:{}\".format(end-start))\n",
    "predictions = lrModel.transform(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba67276-1771-4105-b024-3437ff499e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee7e91-735a-4b2a-81d6-2a62d6430944",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.getMetricName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3998cca0-67c2-45d2-b4f5-999b4e9d38be",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e8ad24-c5b4-4aed-89de-9fab1b736389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a82ae67-8660-44b6-ae2a-15c8aaf9df10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "x = [1,2,3,4]\n",
    "y = [one,two,three,four]\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(x,y)\n",
    "\n",
    "plt.xlabel(\"number of cores\")\n",
    "plt.ylabel(\"Time taken (seconds)\")\n",
    "plt.savefig(\"strong_scaling.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dd30584-6773-444a-ab84-356488f270ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/25 09:27:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/03/25 09:27:52 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "22/03/25 09:27:54 WARN ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "from pyspark import SparkConf\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"spark://192.168.2.6:7077\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "    .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"300s\")\\\n",
    "    .config(\"spark.executor.cores\",1)\\\n",
    "    .config(\"spark.cores.max\", 1)\\\n",
    "    .config(\"spark.driver.port\",9998)\\\n",
    "    .config(\"spark.blockManager.port\",10005)\\\n",
    "    .appName(\"Group2\")\\\n",
    "    .getOrCreate()\n",
    "sc = spark_session.sparkContext\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fd4c50a-a9e7-47ff-b685-e1040fb65276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/25 09:28:25 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"hdfs://192.168.2.6:9000/user/ubuntu/input/RC_2005-12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98ae3eb3-b41b-4bd2-b563-5fcc07d12ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(author='frjo', author_flair_css_class=None, author_flair_text=None, body='A look at Vietnam and Mexico exposes the myth of market liberalisation.', controversiality=0, created_utc=1134365188, distinguished=None, edited=False, gilded=0, id='c13', link_id='t3_17863', parent_id='t3_17863', retrieved_on=1473738411, score=2, stickied=False, subreddit='reddit.com', subreddit_id='t5_6', ups=2),\n",
       " Row(author='zse7zse', author_flair_css_class=None, author_flair_text=None, body='The site states \"What can I use it for? Meeting notes, Reports, technical specs Sign-up sheets, proposals and much more...\", just like any other new breeed of sites that want us to store everything we have on the web. And they even guarantee multiple levels of security and encryption etc. But what prevents these web site operators fom accessing and/or stealing Meeting notes, Reports, technical specs Sign-up sheets, proposals and much more, for competitive or personal gains...? I am pretty sure that most of them are honest, but what\\'s there to prevent me from setting up a good useful site and stealing all your data? Call me paranoid - I am.', controversiality=0, created_utc=1134365725, distinguished=None, edited=False, gilded=0, id='c14', link_id='t3_17866', parent_id='t3_17866', retrieved_on=1473738411, score=1, stickied=False, subreddit='reddit.com', subreddit_id='t5_6', ups=1),\n",
       " Row(author='[deleted]', author_flair_css_class=None, author_flair_text=None, body='Jython related topics by Frank Wierzbicki', controversiality=0, created_utc=1134366848, distinguished=None, edited=False, gilded=0, id='c15', link_id='t3_17869', parent_id='t3_17869', retrieved_on=1473738411, score=0, stickied=False, subreddit='reddit.com', subreddit_id='t5_6', ups=0),\n",
       " Row(author='[deleted]', author_flair_css_class=None, author_flair_text=None, body='[deleted]', controversiality=0, created_utc=1134367660, distinguished=None, edited=False, gilded=0, id='c16', link_id='t3_17870', parent_id='t3_17870', retrieved_on=1473738411, score=1, stickied=False, subreddit='reddit.com', subreddit_id='t5_6', ups=1),\n",
       " Row(author='rjoseph', author_flair_css_class=None, author_flair_text=None, body='Saft is by far the best extension you could tak onto your Safari', controversiality=0, created_utc=1134367754, distinguished=None, edited=False, gilded=0, id='c17', link_id='t3_17817', parent_id='t3_17817', retrieved_on=1473738411, score=1, stickied=False, subreddit='reddit.com', subreddit_id='t5_6', ups=1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5f8d33-d807-4634-a582-e86f1f44567b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
